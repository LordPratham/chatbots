{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirement.txt\n",
    "#print(\"requirements done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mkuma\\anaconda3\\envs\\cap\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyAwHWjFQqU-3pdzDzsvFb0awAz2u5RpM-8\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env\n",
    "load_dotenv(\".env\")\n",
    "print(os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books directory: ./cleaned_txt\n",
      "Persistent directory: ./db\\chroma_db_with_metadata\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define the directory containing the text files and the persistent directory\n",
    "\n",
    "books_dir = './cleaned_txt'\n",
    "db_dir = os.path.join('./', \"db\")\n",
    "persistent_directory = os.path.join(db_dir, \"chroma_db_with_metadata\")\n",
    "\n",
    "print(f\"Books directory: {books_dir}\")\n",
    "print(f\"Persistent directory: {persistent_directory}\")\n",
    "\n",
    "def create_vector_database(embeddings,persistent_directory):\n",
    "    # Check if the Chroma vector store already exists\n",
    "    if not os.path.exists(persistent_directory):\n",
    "        print(\"Persistent directory does not exist. Initializing vector store...\")\n",
    "\n",
    "        # Ensure the books directory exists\n",
    "        if not os.path.exists(books_dir):\n",
    "            raise FileNotFoundError(\n",
    "                f\"The directory {books_dir} does not exist. Please check the path.\"\n",
    "            )\n",
    "\n",
    "        # List all text files in the directory\n",
    "        book_files = [f for f in os.listdir(books_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "        # Read the text content from each file and store it with metadata\n",
    "        documents = []\n",
    "        for book_file in book_files:\n",
    "            file_path = os.path.join(books_dir, book_file)\n",
    "            loader = TextLoader(file_path)\n",
    "            book_docs = loader.load()\n",
    "            for doc in book_docs:\n",
    "                # Add metadata to each document indicating its source\n",
    "                doc.metadata = {\"source\": book_file}\n",
    "                documents.append(doc)\n",
    "\n",
    "        # Split the documents into chunks\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "\n",
    "        # Display information about the split documents\n",
    "        print(\"\\n--- Document Chunks Information ---\")\n",
    "        print(f\"Number of document chunks: {len(docs)}\")\n",
    "\n",
    "        # Create embeddings\n",
    "        print(\"\\n--- Creating embeddings ---\")\n",
    "        # Update to a valid embedding model if needed\n",
    "        print(\"\\n--- Finished creating embeddings ---\")\n",
    "\n",
    "        # Create the vector store and persist it\n",
    "        print(\"\\n--- Creating and persisting vector store ---\")\n",
    "        db = Chroma.from_documents(\n",
    "            docs, embeddings, persist_directory=persistent_directory)\n",
    "        print(\"\\n--- Finished creating and persisting vector store ---\")\n",
    "\n",
    "    else:\n",
    "        print(\"Vector store already exists. No need to initialize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the AI! Type 'exit' to end the conversation.\n",
      "Bye....\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\mkuma\\anaconda3\\envs\\cap\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\mkuma\\anaconda3\\envs\\cap\\lib\\site-packages (from langdetect) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting deep_translator\n",
      "  Using cached deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\users\\mkuma\\anaconda3\\envs\\cap\\lib\\site-packages (from deep_translator) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\mkuma\\anaconda3\\envs\\cap\\lib\\site-packages (from deep_translator) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mkuma\\anaconda3\\envs\\cap\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mkuma\\anaconda3\\envs\\cap\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mkuma\\anaconda3\\envs\\cap\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mkuma\\anaconda3\\envs\\cap\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mkuma\\anaconda3\\envs\\cap\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2024.12.14)\n",
      "Using cached deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
      "Installing collected packages: deep_translator\n",
      "Successfully installed deep_translator-1.11.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install langdetect\n",
    "# %pip install deep_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "# Ensure consistent results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        # Detect the language of the input text\n",
    "        language_code = detect(text)\n",
    "        return language_code\n",
    "    except Exception as e:\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "def english_to_punjabi(english_text):\n",
    "    return GoogleTranslator(source='en', target='pa').translate(english_text)\n",
    "\n",
    "def punjabi_to_english(punjabi_text):\n",
    "    return GoogleTranslator(source='pa', target='en').translate(punjabi_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = os.path.join('./', \"db\")\n",
    "persistent_directory = os.path.join(db_dir, \"extended_chroma_db_with_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store already exists. No need to initialize.\n"
     ]
    }
   ],
   "source": [
    "create_vector_database(huggingface_embeddings,persistent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the AI! Type 'exit' to end the conversation.\n",
      "Hello\n",
      "AI: Hello! How can I help you today?\n",
      "\n",
      "What is wheat?\n",
      "AI: Wheat, scientifically known as *Triticum*, is an annual grass belonging to the family Poaceae (Gramineae).  It's a staple food for a large portion of the world's population, used to make flour for bread, pasta, and other products.  The grain itself contains carbohydrates, protein, fat, minerals, fiber, and moisture.  Wheat straw is also used as animal feed and for various other purposes. (Source: Status Paper on Wheat)\n",
      "\n",
      "Bye....\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Define the persistent directory\n",
    "# current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_with_metadata\")\n",
    "# persistent_directory = \".db/chroma_db_with_metadata\"\n",
    "persistent_directory = \"db/extended_chroma_db_with_metadata\"\n",
    "\n",
    "# Define the embedding model\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# huggingface_embeddings = HuggingFaceEmbeddings(\n",
    "    # model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "# )\n",
    "# huggingface_embeddings=pickle.load(open(\"huggingface_embeddings.pkl\",\"rb\"))\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "# Load the existing vector store with the embedding function\n",
    "db = Chroma(persist_directory=persistent_directory, embedding_function=huggingface_embeddings)\n",
    "\n",
    "# Create a retriever for querying the vector store\n",
    "# `search_type` specifies the type of search (e.g., similarity)\n",
    "# `search_kwargs` contains additional arguments for the search (e.g., number of results to return)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "# Create a google model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# Contextualize question prompt\n",
    "# This system prompt helps the AI understand that it should reformulate the question\n",
    "# based on the chat history to make it a standalone question\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, just \"\n",
    "    \"reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "# Create a prompt template for contextualizing questions\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a history-aware retriever\n",
    "# This uses the LLM to help reformulate the question based on chat history\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "# Answer question prompt\n",
    "# This system prompt helps the AI understand that it should provide concise answers\n",
    "# based on the retrieved context and indicates what to do if the answer is unknown\n",
    "qa_system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. Use \"\n",
    "    \"the following pieces of retrieved context to answer the \"\n",
    "    \"question. If you don't know the answer, just say that you \"\n",
    "    \"don't know. Use optimal number of sentences to answer the question. \"\n",
    "    \"Provide the source as well. \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create a prompt template for answering questions\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a chain to combine documents for question answering\n",
    "# `create_stuff_documents_chain` feeds all retrieved context into the LLM\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Create a retrieval chain that combines the history-aware retriever and the question answering chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "# Function to simulate a continual chat punjabi to punjabi\n",
    "def continual_chat():\n",
    "    print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
    "    chat_history = []  # Collect chat history here (a sequence of messages)\n",
    "\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Bye....\")\n",
    "            break\n",
    "\n",
    "        language=detect_language(query)\n",
    "        \n",
    "        if language.lower()==\"pa\":\n",
    "            query = punjabi_to_english(str(query))\n",
    "        \n",
    "            print(query)\n",
    "        else:\n",
    "            print(query)\n",
    "\n",
    "        # Process the user's query through the retrieval chain\n",
    "        result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "        # Display the AI's response\n",
    "        \n",
    "        if language.lower()==\"pa\":\n",
    "            punjabi_response = english_to_punjabi(str(result['answer']))\n",
    "    \n",
    "            print(f\"AI: {punjabi_response}\")\n",
    "        else:\n",
    "            print(\"AI:\", result['answer'])\n",
    "\n",
    "\n",
    "        # Update the chat history\n",
    "#         chat_history.append(HumanMessage(content=query))\n",
    "#         chat_history.append(SystemMessage(content=result[\"answer\"]))\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        chat_history.append(AIMessage(content=result[\"answer\"]))\n",
    "#         print(chat_history)\n",
    "\n",
    "\n",
    "# Main function to start the continual chat\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
