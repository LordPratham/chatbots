{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"requirements done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# Load environment variables from .env\n",
    "# load_dotenv(\".env\")\n",
    "# print(os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define the directory containing the text files and the persistent directory\n",
    "\n",
    "books_dir = './cleaned_txt'\n",
    "db_dir = os.path.join('./', \"db\")\n",
    "persistent_directory = os.path.join(db_dir, \"chroma_db_with_metadata\")\n",
    "\n",
    "print(f\"Books directory: {books_dir}\")\n",
    "print(f\"Persistent directory: {persistent_directory}\")\n",
    "\n",
    "def create_vector_database(embeddings,persistent_directory):\n",
    "    # Check if the Chroma vector store already exists\n",
    "    if not os.path.exists(persistent_directory):\n",
    "        print(\"Persistent directory does not exist. Initializing vector store...\")\n",
    "\n",
    "        # Ensure the books directory exists\n",
    "        if not os.path.exists(books_dir):\n",
    "            raise FileNotFoundError(\n",
    "                f\"The directory {books_dir} does not exist. Please check the path.\"\n",
    "            )\n",
    "\n",
    "        # List all text files in the directory\n",
    "        book_files = [f for f in os.listdir(books_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "        # Read the text content from each file and store it with metadata\n",
    "        documents = []\n",
    "        for book_file in book_files:\n",
    "            file_path = os.path.join(books_dir, book_file)\n",
    "            loader = TextLoader(file_path)\n",
    "            book_docs = loader.load()\n",
    "            for doc in book_docs:\n",
    "                # Add metadata to each document indicating its source\n",
    "                doc.metadata = {\"source\": book_file}\n",
    "                documents.append(doc)\n",
    "\n",
    "        # Split the documents into chunks\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "\n",
    "        # Display information about the split documents\n",
    "        print(\"\\n--- Document Chunks Information ---\")\n",
    "        print(f\"Number of document chunks: {len(docs)}\")\n",
    "\n",
    "        # Create embeddings\n",
    "        print(\"\\n--- Creating embeddings ---\")\n",
    "        # Update to a valid embedding model if needed\n",
    "        print(\"\\n--- Finished creating embeddings ---\")\n",
    "\n",
    "        # Create the vector store and persist it\n",
    "        print(\"\\n--- Creating and persisting vector store ---\")\n",
    "        db = Chroma.from_documents(\n",
    "            docs, embeddings, persist_directory=persistent_directory)\n",
    "        print(\"\\n--- Finished creating and persisting vector store ---\")\n",
    "\n",
    "    else:\n",
    "        print(\"Vector store already exists. No need to initialize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the persistent directory\n",
    "# current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_with_metadata\")\n",
    "persistent_directory = \".db/chroma_db_with_metadata\"\n",
    "\n",
    "# Define the embedding model\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "# Load the existing vector store with the embedding function\n",
    "db = Chroma(persist_directory=persistent_directory, embedding_function=huggingface_embeddings)\n",
    "\n",
    "# Create a retriever for querying the vector store\n",
    "# `search_type` specifies the type of search (e.g., similarity)\n",
    "# `search_kwargs` contains additional arguments for the search (e.g., number of results to return)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "# Create a google model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# Contextualize question prompt\n",
    "# This system prompt helps the AI understand that it should reformulate the question\n",
    "# based on the chat history to make it a standalone question\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, just \"\n",
    "    \"reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "# Create a prompt template for contextualizing questions\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a history-aware retriever\n",
    "# This uses the LLM to help reformulate the question based on chat history\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "# Answer question prompt\n",
    "# This system prompt helps the AI understand that it should provide concise answers\n",
    "# based on the retrieved context and indicates what to do if the answer is unknown\n",
    "qa_system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. Use \"\n",
    "    \"the following pieces of retrieved context to answer the \"\n",
    "    \"question. If you don't know the answer, just say that you \"\n",
    "    \"don't know. Use optimal number of sentences to answer the question. \"\n",
    "    \"Provide the source as well. \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create a prompt template for answering questions\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a chain to combine documents for question answering\n",
    "# `create_stuff_documents_chain` feeds all retrieved context into the LLM\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Create a retrieval chain that combines the history-aware retriever and the question answering chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "# Function to simulate a continual chat\n",
    "def continual_chat():\n",
    "    print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
    "    chat_history = []  # Collect chat history here (a sequence of messages)\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Bye....\")\n",
    "            break\n",
    "        # Process the user's query through the retrieval chain\n",
    "        result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "        # Display the AI's response\n",
    "        print(f\"AI: {result['answer']}\")\n",
    "        # Update the chat history\n",
    "#         chat_history.append(HumanMessage(content=query))\n",
    "#         chat_history.append(SystemMessage(content=result[\"answer\"]))\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        chat_history.append(AIMessage(content=result[\"answer\"]))\n",
    "#         print(chat_history)\n",
    "\n",
    "\n",
    "# Main function to start the continual chat\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "# Ensure consistent results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        # Detect the language of the input text\n",
    "        language_code = detect(text)\n",
    "        return language_code\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Test the function with Punjabi text\n",
    "punjabi_text = \"ਤੁਸੀਂ ਕਿਵੇਂ ਹੋ?\"\n",
    "detected_language = detect_language(punjabi_text)\n",
    "print(f\"The detected language code is: {detected_language}\")\n",
    "\n",
    "# You can also test with English\n",
    "english_text = \"How are you?\"\n",
    "detected_language = detect_language(english_text)\n",
    "print(f\"The detected language code is: {detected_language}\")\n",
    "\n",
    "# Test with Hindi\n",
    "hindi_text = \"आप कैसे हैं?\"\n",
    "detected_language = detect_language(hindi_text)\n",
    "print(f\"The detected language code is: {detected_language}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deep_translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "def english_to_punjabi(english_text):\n",
    "    return GoogleTranslator(source='en', target='pa').translate(english_text)\n",
    "\n",
    "def punjabi_to_english(punjabi_text):\n",
    "    return GoogleTranslator(source='pa', target='en').translate(punjabi_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = os.path.join('./', \"db\")\n",
    "persistent_directory = os.path.join(db_dir, \"extended_chroma_db_with_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vector_database(huggingface_embeddings,persistent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Define the persistent directory\n",
    "# current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_with_metadata\")\n",
    "# persistent_directory = \".db/chroma_db_with_metadata\"\n",
    "persistent_directory = \"db/extended_chroma_db_with_metadata\"\n",
    "\n",
    "# Define the embedding model\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# huggingface_embeddings = HuggingFaceEmbeddings(\n",
    "    # model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "# )\n",
    "# huggingface_embeddings=pickle.load(open(\"huggingface_embeddings.pkl\",\"rb\"))\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "# Load the existing vector store with the embedding function\n",
    "db = Chroma(persist_directory=persistent_directory, embedding_function=huggingface_embeddings)\n",
    "\n",
    "# Create a retriever for querying the vector store\n",
    "# `search_type` specifies the type of search (e.g., similarity)\n",
    "# `search_kwargs` contains additional arguments for the search (e.g., number of results to return)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "# Create a google model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# Contextualize question prompt\n",
    "# This system prompt helps the AI understand that it should reformulate the question\n",
    "# based on the chat history to make it a standalone question\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, just \"\n",
    "    \"reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "# Create a prompt template for contextualizing questions\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a history-aware retriever\n",
    "# This uses the LLM to help reformulate the question based on chat history\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "# Answer question prompt\n",
    "# This system prompt helps the AI understand that it should provide concise answers\n",
    "# based on the retrieved context and indicates what to do if the answer is unknown\n",
    "qa_system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. Use \"\n",
    "    \"the following pieces of retrieved context to answer the \"\n",
    "    \"question. If you don't know the answer, just say that you \"\n",
    "    \"don't know. Use optimal number of sentences to answer the question. \"\n",
    "    \"Provide the source as well. \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create a prompt template for answering questions\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a chain to combine documents for question answering\n",
    "# `create_stuff_documents_chain` feeds all retrieved context into the LLM\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Create a retrieval chain that combines the history-aware retriever and the question answering chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "# Function to simulate a continual chat punjabi to punjabi\n",
    "def continual_chat():\n",
    "    print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
    "    chat_history = []  # Collect chat history here (a sequence of messages)\n",
    "\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Bye....\")\n",
    "            break\n",
    "\n",
    "        language=detect_language(query)\n",
    "        \n",
    "        if language.lower()==\"pa\":\n",
    "            query = punjabi_to_english(str(query))\n",
    "        \n",
    "            print(query)\n",
    "        else:\n",
    "            print(query)\n",
    "\n",
    "        # Process the user's query through the retrieval chain\n",
    "        result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "        # Display the AI's response\n",
    "        \n",
    "        if language.lower()==\"pa\":\n",
    "            punjabi_response = english_to_punjabi(str(result['answer']))\n",
    "    \n",
    "            print(f\"AI: {punjabi_response}\")\n",
    "        else:\n",
    "            print(\"AI:\", result['answer'])\n",
    "\n",
    "\n",
    "        # Update the chat history\n",
    "#         chat_history.append(HumanMessage(content=query))\n",
    "#         chat_history.append(SystemMessage(content=result[\"answer\"]))\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        chat_history.append(AIMessage(content=result[\"answer\"]))\n",
    "#         print(chat_history)\n",
    "\n",
    "\n",
    "# Main function to start the continual chat\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"requirements done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# Load environment variables from .env\n",
    "load_dotenv(\".env\")\n",
    "# print(os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define the directory containing the text files and the persistent directory\n",
    "\n",
    "books_dir = './cleaned_txt'\n",
    "db_dir = os.path.join('./', \"db\")\n",
    "persistent_directory = os.path.join(db_dir, \"chroma_db_with_metadata\")\n",
    "\n",
    "print(f\"Books directory: {books_dir}\")\n",
    "print(f\"Persistent directory: {persistent_directory}\")\n",
    "\n",
    "def create_vector_database(embeddings,persistent_directory):\n",
    "    # Check if the Chroma vector store already exists\n",
    "    if not os.path.exists(persistent_directory):\n",
    "        print(\"Persistent directory does not exist. Initializing vector store...\")\n",
    "\n",
    "        # Ensure the books directory exists\n",
    "        if not os.path.exists(books_dir):\n",
    "            raise FileNotFoundError(\n",
    "                f\"The directory {books_dir} does not exist. Please check the path.\"\n",
    "            )\n",
    "\n",
    "        # List all text files in the directory\n",
    "        book_files = [f for f in os.listdir(books_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "        # Read the text content from each file and store it with metadata\n",
    "        documents = []\n",
    "        for book_file in book_files:\n",
    "            file_path = os.path.join(books_dir, book_file)\n",
    "            loader = TextLoader(file_path)\n",
    "            book_docs = loader.load()\n",
    "            for doc in book_docs:\n",
    "                # Add metadata to each document indicating its source\n",
    "                doc.metadata = {\"source\": book_file}\n",
    "                documents.append(doc)\n",
    "\n",
    "        # Split the documents into chunks\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "\n",
    "        # Display information about the split documents\n",
    "        print(\"\\n--- Document Chunks Information ---\")\n",
    "        print(f\"Number of document chunks: {len(docs)}\")\n",
    "\n",
    "        # Create embeddings\n",
    "        print(\"\\n--- Creating embeddings ---\")\n",
    "        # Update to a valid embedding model if needed\n",
    "        print(\"\\n--- Finished creating embeddings ---\")\n",
    "\n",
    "        # Create the vector store and persist it\n",
    "        print(\"\\n--- Creating and persisting vector store ---\")\n",
    "        db = Chroma.from_documents(\n",
    "            docs, embeddings, persist_directory=persistent_directory)\n",
    "        print(\"\\n--- Finished creating and persisting vector store ---\")\n",
    "\n",
    "    else:\n",
    "        print(\"Vector store already exists. No need to initialize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the persistent directory\n",
    "# current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_with_metadata\")\n",
    "persistent_directory = \".db/chroma_db_with_metadata\"\n",
    "\n",
    "# Define the embedding model\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "# Load the existing vector store with the embedding function\n",
    "db = Chroma(persist_directory=persistent_directory, embedding_function=huggingface_embeddings)\n",
    "\n",
    "# Create a retriever for querying the vector store\n",
    "# `search_type` specifies the type of search (e.g., similarity)\n",
    "# `search_kwargs` contains additional arguments for the search (e.g., number of results to return)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "# Create a google model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# Contextualize question prompt\n",
    "# This system prompt helps the AI understand that it should reformulate the question\n",
    "# based on the chat history to make it a standalone question\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, just \"\n",
    "    \"reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "# Create a prompt template for contextualizing questions\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a history-aware retriever\n",
    "# This uses the LLM to help reformulate the question based on chat history\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "# Answer question prompt\n",
    "# This system prompt helps the AI understand that it should provide concise answers\n",
    "# based on the retrieved context and indicates what to do if the answer is unknown\n",
    "qa_system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. Use \"\n",
    "    \"the following pieces of retrieved context to answer the \"\n",
    "    \"question. If you don't know the answer, just say that you \"\n",
    "    \"don't know. Use optimal number of sentences to answer the question. \"\n",
    "    \"Provide the source as well. \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create a prompt template for answering questions\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a chain to combine documents for question answering\n",
    "# `create_stuff_documents_chain` feeds all retrieved context into the LLM\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Create a retrieval chain that combines the history-aware retriever and the question answering chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "# Function to simulate a continual chat\n",
    "def continual_chat():\n",
    "    print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
    "    chat_history = []  # Collect chat history here (a sequence of messages)\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Bye....\")\n",
    "            break\n",
    "        # Process the user's query through the retrieval chain\n",
    "        result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "        # Display the AI's response\n",
    "        print(f\"AI: {result['answer']}\")\n",
    "        # Update the chat history\n",
    "#         chat_history.append(HumanMessage(content=query))\n",
    "#         chat_history.append(SystemMessage(content=result[\"answer\"]))\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        chat_history.append(AIMessage(content=result[\"answer\"]))\n",
    "#         print(chat_history)\n",
    "\n",
    "\n",
    "# Main function to start the continual chat\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "# Ensure consistent results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        # Detect the language of the input text\n",
    "        language_code = detect(text)\n",
    "        return language_code\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Test the function with Punjabi text\n",
    "punjabi_text = \"ਤੁਸੀਂ ਕਿਵੇਂ ਹੋ?\"\n",
    "detected_language = detect_language(punjabi_text)\n",
    "print(f\"The detected language code is: {detected_language}\")\n",
    "\n",
    "# You can also test with English\n",
    "english_text = \"How are you?\"\n",
    "detected_language = detect_language(english_text)\n",
    "print(f\"The detected language code is: {detected_language}\")\n",
    "\n",
    "# Test with Hindi\n",
    "hindi_text = \"आप कैसे हैं?\"\n",
    "detected_language = detect_language(hindi_text)\n",
    "print(f\"The detected language code is: {detected_language}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deep_translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "def english_to_punjabi(english_text):\n",
    "    return GoogleTranslator(source='en', target='pa').translate(english_text)\n",
    "\n",
    "def punjabi_to_english(punjabi_text):\n",
    "    return GoogleTranslator(source='pa', target='en').translate(punjabi_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = os.path.join('./', \"db\")\n",
    "persistent_directory = os.path.join(db_dir, \"extended_chroma_db_with_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vector_database(huggingface_embeddings,persistent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Define the persistent directory\n",
    "# current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_with_metadata\")\n",
    "# persistent_directory = \".db/chroma_db_with_metadata\"\n",
    "persistent_directory = \"db/extended_chroma_db_with_metadata\"\n",
    "\n",
    "# Define the embedding model\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# huggingface_embeddings = HuggingFaceEmbeddings(\n",
    "    # model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "# )\n",
    "huggingface_embeddings=pickle.load(open(\"huggingface_embeddings.pkl\",\"rb\"))\n",
    "# Load the existing vector store with the embedding function\n",
    "db = Chroma(persist_directory=persistent_directory, embedding_function=huggingface_embeddings)\n",
    "\n",
    "# Create a retriever for querying the vector store\n",
    "# `search_type` specifies the type of search (e.g., similarity)\n",
    "# `search_kwargs` contains additional arguments for the search (e.g., number of results to return)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "# Create a google model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# Contextualize question prompt\n",
    "# This system prompt helps the AI understand that it should reformulate the question\n",
    "# based on the chat history to make it a standalone question\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, just \"\n",
    "    \"reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "# Create a prompt template for contextualizing questions\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a history-aware retriever\n",
    "# This uses the LLM to help reformulate the question based on chat history\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "# Answer question prompt\n",
    "# This system prompt helps the AI understand that it should provide concise answers\n",
    "# based on the retrieved context and indicates what to do if the answer is unknown\n",
    "qa_system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. Use \"\n",
    "    \"the following pieces of retrieved context to answer the \"\n",
    "    \"question. If you don't know the answer, just say that you \"\n",
    "    \"don't know. Use optimal number of sentences to answer the question. \"\n",
    "    \"Provide the source as well. \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create a prompt template for answering questions\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a chain to combine documents for question answering\n",
    "# `create_stuff_documents_chain` feeds all retrieved context into the LLM\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Create a retrieval chain that combines the history-aware retriever and the question answering chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "# Function to simulate a continual chat punjabi to punjabi\n",
    "def continual_chat():\n",
    "    print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
    "    chat_history = []  # Collect chat history here (a sequence of messages)\n",
    "\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Bye....\")\n",
    "            break\n",
    "\n",
    "        language=detect_language(query)\n",
    "        \n",
    "        if language.lower()==\"pa\":\n",
    "            query = punjabi_to_english(str(query))\n",
    "        \n",
    "            print(query)\n",
    "        else:\n",
    "            print(query)\n",
    "\n",
    "        # Process the user's query through the retrieval chain\n",
    "        result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "        # Display the AI's response\n",
    "        \n",
    "        if language.lower()==\"pa\":\n",
    "            punjabi_response = english_to_punjabi(str(result['answer']))\n",
    "    \n",
    "            print(f\"AI: {punjabi_response}\")\n",
    "        else:\n",
    "            print(\"AI:\", result['answer'])\n",
    "\n",
    "\n",
    "        # Update the chat history\n",
    "#         chat_history.append(HumanMessage(content=query))\n",
    "#         chat_history.append(SystemMessage(content=result[\"answer\"]))\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        chat_history.append(AIMessage(content=result[\"answer\"]))\n",
    "#         print(chat_history)\n",
    "\n",
    "\n",
    "# Main function to start the continual chat\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
